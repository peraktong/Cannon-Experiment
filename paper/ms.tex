\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.

\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath, amsthm, amssymb, amsfonts}

\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}


%SetFonts

%SetFonts


\title{Optimize the Cannon 2}
\author[1]{Jason Cao\thanks{jc6933@nyu.edu}}
\author[2]{David W. hogg\thanks{david.hogg@nyu.edu}}

\affil[1]{Department of Physics,  New York University}
\affil[2]{NYU Physics - Center for Cosmology and Particle Physics
NYU Center for Data Science
Max-Planck-Institut fuer Astronomie }

\renewcommand\Authands{ and }

  
\date{\today}						% Activate to display a given date or no date



\begin{document}
\maketitle


\section{\label{sec:level1}Introduction}


Coming soon...
 
 %% second paragraph
 
 \section{\label{sec:level1}Assumption and method}
 \subsection{\label{sec:level2}Optimize the spectrum}

 
Suppose we have a trained model. The training set contains 548 stellar and each stellar spectrum has 8575 pixels. Every pixel is related to a specific wave length. It belongs to APOGEE data release 10 and has a good quality. These stellar are the very same objects as used by the APOGEE survey for the calibration of DR10. Each of them has at least nine stellar labels. But in the training step, we only use three of them $T_{eff}$, $log$ g  and $\left[Fe/H\right]$. They span the range of $3500K <T_{eff}< 5300K$, $0 <log$ $g< 5$ and $-2.5 <\left[Fe/H\right]< 0.45$.
]

The model we adopt
 \[y_{jn} = v(l_n) \cdot \theta_{j} + e_{jn} (1)\]                      
 
 Where \(y_{jn}\) is the spectrum data for star n at wavelength pixel j. 
 \(v(l_n)\) is the vectorizing function. 
The input \(l_n\) is the label list of length K for star n and the  output 
\(v(l_n)\) is a vector of length D (D is bigger than K).
\( \theta_{j}\) is a vector of length D of parameters which controlling the model at wavelength pixel j.
\( e_{jn}\) is a noise draw or residual at pixel j for star n.


\(l_n^{inf}\) and \( \theta_{j}\) are available after the training step.
Infer spectrum by using:
 \[y_{jn}^{inf} = v(l_n^{inf}) \cdot \theta_{j}  (2)\]
 
 This is how to obtain the synthesized spectrum by using the Cannon 2. We also call it inferred flux(spectrum).

The Cannon 2 can predict the spectrum pretty well. But we can still improve it.
\bigskip

From the inferred spectrum \(y_{jn}^{inf}\), we optimize it and generate a better synthesized spectrum, which we call \(y_{jn}^{opt}\).
\bigskip

The method we use is to represent the spectrum at pixel j by using the inferred spectrum at pixel j-1,pixel j and pixel j+1.(If the pixel is out of range, set the flux as 1). The data we use is the same as the training step.
\bigskip

The new spectrum, which is the optimized spectrum, is represented by:
\[y_{j,n}^{opt}=a\cdot y_{j-1,n}^{inf}+b\cdot y_{j,n}^{inf}+c\cdot y_{j+1,n}^{inf}(3)\]
\bigskip

We use the least chi-squared method. So our objective function is the chi-squared between the optimized spectrum and the spectrum data. 
\bigskip

$F_{obj} (a,b,c)$= \[\sum_{n=1,j=1}^{n=N,j=J} {(y_{jn} - y_{jn}^{opt})^2 \over{\sigma_{jn}^2}} \]  (4)
\bigskip

$\sigma_{jn}^2$ is the data uncertainty for star n at pixel j. The number of stellar and pixel each star are N and J. Here N and J are the number of the stellar and the number of pixel each star.  Write the objective function in the form of matrix can make it simpler. So we introduce several matrix: Y, A, C and X, which are defined as followed:

% insert matrix

\[
Y
=
\begin{bmatrix}
    Y_1&Y_2 &\dots&Y_{N}\\
    
\end{bmatrix}
\]

where
\[
 Y_n
=
\begin{bmatrix}
    y_{1,n} \\
    y_{2,n} \\
    \vdots  \\
    y_{J,n}  \\
    
\end{bmatrix}
\]

% A
\[
A
=
\begin{bmatrix}
    A_1&A_2 &\dots&A_{N}\\
    
\end{bmatrix}
\]

where
\[
A_n
=
\begin{bmatrix}
    
    1 & y_{1,n} ^{inf} & y_{2,n} ^{inf}\\
    y_{1,n}^{inf}&y_{2,n}^{inf}& y_{3,n}^{inf}  \\
    \vdots & \vdots & \vdots \\
    y_{J-1,n}^{inf}&y_{J,n}^{inf}& 1  \\

\end{bmatrix}
\]



% sigma-C

\[
C
=
\begin{bmatrix}
    C_1&C_2 &\dots&C_{N}\\
    
\end{bmatrix}
\]

where
\[
C_n
=
\begin{bmatrix}
    
    \sigma_{1,n}^2&0&\dots&0\\
    0& \sigma_{2,n}^2&\dots&0 \\
    \vdots & \vdots & \vdots \\
    0&\dots&0&\sigma_{J,n}^2  \\

\end{bmatrix}
\]
 
\bigskip

\(X = 
 \begin{bmatrix}
    
    a\\
    b \\
    c \\

\end{bmatrix}
\)


We cut matrix Y, A and C into N smaller matrix so the calculation speed is higher. 
\bigskip

Now the objective function can be described as:
$F_{obj} (a,b,c)$= \[\sum_{n=1}^{N} (Y_n-A_n X)^TC^{-1}(Y_n-A_n X) (5)\]  
\bigskip

And the Jacobian Matrix of the objective function with respect to parameters is


J = $\partial F_{obj}\over \partial X$ \[=\sum_{n=1}^{n=N}2\cdot [(Y_n-A_nX)^TC^{-1}]\cdot (-A_n) (6)\]   
\bigskip

The dimension of the Jacobian Matrix is 1*3. To minimize the objective function, let the Jacobian Matrix be 0 and we have:

\[X = [\sum_{n=1}^{n=N}A_n^TC_n^{-1}A]^{-1} \cdot [\sum_{n=1}^{n=N}A^TC_n^{-1}Y_n](7)\]
\bigskip

By using (7) we can calculate a, b and c. b should be much bigger than both a and c. And a+b+c should be equal to 1.


Calculate the optimized spectrum \( y_{j,n}^{opt}\) by using (3). The Experiment 1 shows that the optimization works.
\bigskip



%If the optimization works, we will optimized \(\theta_{j}\) besides a, b and c, which will come soon.
%It works!

 \subsection{\label{sec:level2}Optimized $\theta_{j}$ and inferred labels}
 After optimizing the inferred spectrum, we can modify $\theta_{j}$ and get a better result. The method is to represent $\theta_j$ by using $\theta_{j-1}$ , $\theta_j$ and $\theta_{j+1}$. If $\theta_j$ is out of range, set it as zero. So the optimized $\theta$ is:
 \[\theta_{j}^{opt}=a\cdot \theta_{j-1}+b\cdot \theta_{j}+c\cdot \theta_{j+1}(8)\]
 \bigskip
 
 Here a,b and c are parameters obtained from spectrum optimization. Then we infer stellar labels by using $\theta_{j}^{opt}$ and generate optimized inferred labels. The data we use is the training set, which includes 548 stellar with 8575 pixel each.
 Finally, compare the chi-squared to check whether the optimization works.


%% Third paragraph
 
 \section{\label{sec:level1}Experiment}
 \subsection{\label{sec:level2}Experiment 1}
 
 Choose four stellar, which are not in the training set. Optimize the spectrum of these four stars and obtain a, b and c.
 
 The result is \(a+b+c = 0.999\). 

(a,b,c) = (0.078, 0.886, 0.035).

The values of the objective function before and after are 177254 and 174031, which decreases by 2\%.

The labels of the four stars are:
 
 Star A  $T_{eff} =4750$  log g =3.0 $\left[Fe/h\right] =0.15$
 
 Star B  $T_{eff} =4849$  log g =2.2 $\left[Fe/h\right] =-1.0$
 
 Star C  $T_{eff} =3614$  log g =0.4 $\left[Fe/h\right] =-0.68$
 
 Star D  $T_{eff} =5003$  log g =2.8 $\left[Fe/h\right] =-0.71$
 
 
 Then plot the spectrum
 
 The black line "Observed" is the observed spectrum from the data  \(y_{jn}\).
 The green line "inferred" is \( y_{j,n}^{inf}\) , which is generated by the Cannon 2.
 The red line "optimized" is \( y_{j,n}^{opt}\), which is generated by (3)
 
\bigskip


 
%\includegraphics[width=170mm]{figure_1.png}
 \centerline{Fig.1 The optimized spectrum and the data spectrum}
\bigskip



%\includegraphics[width=170mm]{figure_2.png}
 \centerline{Fig.2 The inferred spectrum and the data spectrum}
 
 

 
Finally compare the chi-square for these four stars of the two methods:
The total chi-squared of the four stars becomes 2\% smaller, which means our method works.
  
 \subsection{\label{sec:level2}Experiment 2}
 Plot the parameters a, b and c vs the number of the star. 
 \bigskip
 
In experiment 1, we use only four stars to obtain parameters a, b and c and the result is pretty good.  What if we optimize more stars? We randomly choose about 1\% of the APOGEE DR10 and make a histogram plot. The number of stars is 639. But now parameters a, b and c are obtained from independent stars, which means the objective function only contain the chi-squared of one star and there will be 639 objective functions. And there will be 639 sets of a, b and c. Then we can plot a histogram of a, b and c to see the distribution of their values.
\bigskip

%\includegraphics[width=170mm]{figure_3.png}

\bigskip


Also. We plot a, b, c against the mean fiber number, which is the average fiber number of the spectrum we optimize. In APOGEE DR 10, the fiber number is between 1 and 640. The parameters we use are from $figure_3$

%\includegraphics[width=170mm]{figure_4.png}
\bigskip

This figure gives us information about the relation between parameters a, b, c and fiber number, which can be used to establish a data-driven spectrum calibration model.



  
  
 %% Fourth paragraph

 \section{\label{sec:level1}Discussion}
 
 Coming soon.



\end{document}  